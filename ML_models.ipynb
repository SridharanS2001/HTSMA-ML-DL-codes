{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Network (ANN)**"
      ],
      "metadata": {
        "id": "07s3y_QINVSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoWi4kCtEgt3",
        "outputId": "dbf11661-4a15-4732-c497-b01183c8a839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 133246.9219 - mse: 133244.7969 - val_loss: 129443.1328 - val_mse: 129440.5391\n",
            "Epoch 2/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 130725.2188 - mse: 130722.3828 - val_loss: 124162.1484 - val_mse: 124158.3438\n",
            "Epoch 3/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 123707.7812 - mse: 123703.5391 - val_loss: 110141.8828 - val_mse: 110135.9766\n",
            "Epoch 4/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 105480.2656 - mse: 105473.6719 - val_loss: 82082.7031 - val_mse: 82073.5000\n",
            "Epoch 5/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76755.0625 - mse: 76744.8828 - val_loss: 43488.1641 - val_mse: 43474.5000\n",
            "Epoch 6/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 37657.0586 - mse: 37642.2461 - val_loss: 15665.9873 - val_mse: 15647.7373\n",
            "Epoch 7/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22237.0645 - mse: 22218.0898 - val_loss: 9058.1104 - val_mse: 9038.1006\n",
            "Epoch 8/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17401.7832 - mse: 17381.8262 - val_loss: 6522.9048 - val_mse: 6503.2148\n",
            "Epoch 9/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14403.7539 - mse: 14384.0176 - val_loss: 5743.7773 - val_mse: 5724.1348\n",
            "Epoch 10/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15183.2266 - mse: 15163.5879 - val_loss: 5205.6514 - val_mse: 5186.0586\n",
            "Epoch 11/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16489.8105 - mse: 16470.1406 - val_loss: 4670.8643 - val_mse: 4651.2070\n",
            "Epoch 12/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17229.9395 - mse: 17210.2598 - val_loss: 4306.9497 - val_mse: 4287.0171\n",
            "Epoch 13/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15907.6318 - mse: 15887.6885 - val_loss: 4489.5757 - val_mse: 4470.0688\n",
            "Epoch 14/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10836.1562 - mse: 10816.7686 - val_loss: 4288.5640 - val_mse: 4268.9121\n",
            "Epoch 15/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10896.4258 - mse: 10876.5693 - val_loss: 4236.2329 - val_mse: 4216.2329\n",
            "Epoch 16/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 12736.5684 - mse: 12716.6289 - val_loss: 4445.1382 - val_mse: 4425.0898\n",
            "Epoch 17/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9765.0410 - mse: 9745.0508 - val_loss: 4898.9727 - val_mse: 4879.4448\n",
            "Epoch 18/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11727.3926 - mse: 11707.9902 - val_loss: 4947.9438 - val_mse: 4928.5659\n",
            "Epoch 19/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8687.5215 - mse: 8668.0098 - val_loss: 4403.5684 - val_mse: 4383.5923\n",
            "Epoch 20/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 11223.4326 - mse: 11203.3916 - val_loss: 4326.1021 - val_mse: 4305.8340\n",
            "Epoch 21/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 13022.4453 - mse: 13002.1016 - val_loss: 4145.0430 - val_mse: 4124.7163\n",
            "Epoch 22/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14411.7432 - mse: 14391.4600 - val_loss: 4092.4153 - val_mse: 4072.4292\n",
            "Epoch 23/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10259.6533 - mse: 10239.7969 - val_loss: 4456.8066 - val_mse: 4436.9507\n",
            "Epoch 24/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11812.2188 - mse: 11792.3496 - val_loss: 4936.0776 - val_mse: 4916.3843\n",
            "Epoch 25/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 17246.3125 - mse: 17226.6230 - val_loss: 4499.8428 - val_mse: 4479.9312\n",
            "Epoch 26/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 14048.9648 - mse: 14028.8291 - val_loss: 4076.1118 - val_mse: 4055.9104\n",
            "Epoch 27/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13381.3633 - mse: 13361.4258 - val_loss: 4812.1919 - val_mse: 4792.6011\n",
            "Epoch 28/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16229.9912 - mse: 16210.2578 - val_loss: 3905.6892 - val_mse: 3885.2993\n",
            "Epoch 29/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15991.3281 - mse: 15970.9160 - val_loss: 4157.1611 - val_mse: 4137.1616\n",
            "Epoch 30/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16001.6729 - mse: 15981.7959 - val_loss: 4420.3198 - val_mse: 4400.4316\n",
            "Epoch 31/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11289.1230 - mse: 11269.0312 - val_loss: 3834.7795 - val_mse: 3814.2102\n",
            "Epoch 32/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12150.7090 - mse: 12130.0527 - val_loss: 3733.1011 - val_mse: 3712.0371\n",
            "Epoch 33/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12026.1836 - mse: 12004.9912 - val_loss: 3626.5420 - val_mse: 3605.5854\n",
            "Epoch 34/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 11802.4834 - mse: 11781.7979 - val_loss: 3784.2639 - val_mse: 3764.0667\n",
            "Epoch 35/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9579.3896 - mse: 9559.1211 - val_loss: 3739.5620 - val_mse: 3719.1431\n",
            "Epoch 36/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13676.2842 - mse: 13655.9824 - val_loss: 3812.6965 - val_mse: 3792.5645\n",
            "Epoch 37/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12137.5879 - mse: 12117.3701 - val_loss: 3992.7000 - val_mse: 3972.5723\n",
            "Epoch 38/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11952.0273 - mse: 11931.8877 - val_loss: 3856.1477 - val_mse: 3835.7039\n",
            "Epoch 39/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 12884.7451 - mse: 12864.2324 - val_loss: 3540.4563 - val_mse: 3519.7312\n",
            "Epoch 40/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13268.6123 - mse: 13247.8887 - val_loss: 3202.3909 - val_mse: 3181.6245\n",
            "Epoch 41/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11946.2793 - mse: 11925.4844 - val_loss: 3292.0740 - val_mse: 3271.5254\n",
            "Epoch 42/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11594.1055 - mse: 11573.5566 - val_loss: 3028.5950 - val_mse: 3007.7324\n",
            "Epoch 43/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12266.4414 - mse: 12245.6855 - val_loss: 3803.5518 - val_mse: 3783.4407\n",
            "Epoch 44/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14096.5146 - mse: 14076.3379 - val_loss: 3302.1438 - val_mse: 3281.2478\n",
            "Epoch 45/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11097.2207 - mse: 11076.1465 - val_loss: 3376.4055 - val_mse: 3355.0671\n",
            "Epoch 46/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13873.3838 - mse: 13852.0762 - val_loss: 3191.2202 - val_mse: 3170.1038\n",
            "Epoch 47/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12936.3994 - mse: 12915.2773 - val_loss: 3123.0928 - val_mse: 3102.0969\n",
            "Epoch 48/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11820.8018 - mse: 11799.9941 - val_loss: 4131.9180 - val_mse: 4111.8662\n",
            "Epoch 49/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10454.1074 - mse: 10434.0801 - val_loss: 3727.0740 - val_mse: 3706.6555\n",
            "Epoch 50/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9561.5195 - mse: 9540.9219 - val_loss: 3450.1790 - val_mse: 3429.2229\n",
            "Epoch 51/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13867.3232 - mse: 13846.4238 - val_loss: 4245.4316 - val_mse: 4224.8799\n",
            "Epoch 52/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11148.9355 - mse: 11128.5010 - val_loss: 5600.4629 - val_mse: 5580.4482\n",
            "Epoch 53/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11768.9258 - mse: 11748.9346 - val_loss: 4813.0894 - val_mse: 4792.5767\n",
            "Epoch 54/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9913.9678 - mse: 9893.2490 - val_loss: 3867.4316 - val_mse: 3845.8853\n",
            "Epoch 55/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12114.8691 - mse: 12093.3125 - val_loss: 3978.7927 - val_mse: 3957.6538\n",
            "Epoch 56/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10945.3047 - mse: 10924.2383 - val_loss: 4642.3208 - val_mse: 4621.7227\n",
            "Epoch 57/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12089.4014 - mse: 12068.8174 - val_loss: 5018.4419 - val_mse: 4997.7896\n",
            "Epoch 58/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12108.3711 - mse: 12087.6855 - val_loss: 4747.6602 - val_mse: 4726.6313\n",
            "Epoch 59/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12735.2998 - mse: 12714.1973 - val_loss: 4366.0073 - val_mse: 4344.9492\n",
            "Epoch 60/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10636.0332 - mse: 10614.9727 - val_loss: 4012.8989 - val_mse: 3991.7229\n",
            "Epoch 61/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15935.1826 - mse: 15913.8779 - val_loss: 3403.4639 - val_mse: 3381.7793\n",
            "Epoch 62/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11121.9258 - mse: 11100.3682 - val_loss: 4237.5586 - val_mse: 4216.9004\n",
            "Epoch 63/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10482.7588 - mse: 10462.0684 - val_loss: 3472.3223 - val_mse: 3450.8462\n",
            "Epoch 64/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15353.0039 - mse: 15331.2002 - val_loss: 3385.5784 - val_mse: 3363.6895\n",
            "Epoch 65/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11575.2002 - mse: 11553.6172 - val_loss: 3787.3105 - val_mse: 3766.1213\n",
            "Epoch 66/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15157.6953 - mse: 15136.3760 - val_loss: 3422.2847 - val_mse: 3400.7886\n",
            "Epoch 67/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10201.1758 - mse: 10179.5488 - val_loss: 3220.5408 - val_mse: 3198.5015\n",
            "Epoch 68/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11168.9658 - mse: 11146.9551 - val_loss: 3297.5579 - val_mse: 3276.1372\n",
            "Epoch 69/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11380.6738 - mse: 11359.3594 - val_loss: 3494.2129 - val_mse: 3472.8083\n",
            "Epoch 70/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9310.9170 - mse: 9289.3516 - val_loss: 3445.4771 - val_mse: 3423.7810\n",
            "Epoch 71/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12943.2012 - mse: 12921.5254 - val_loss: 3989.7031 - val_mse: 3968.4224\n",
            "Epoch 72/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9460.4023 - mse: 9439.1904 - val_loss: 3819.3420 - val_mse: 3797.6226\n",
            "Epoch 73/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13579.3779 - mse: 13557.5938 - val_loss: 3992.2952 - val_mse: 3970.7927\n",
            "Epoch 74/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10008.6211 - mse: 9987.0234 - val_loss: 3921.2910 - val_mse: 3899.7590\n",
            "Epoch 75/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12549.6523 - mse: 12528.5635 - val_loss: 5922.1968 - val_mse: 5901.8452\n",
            "Epoch 76/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11379.0234 - mse: 11358.5010 - val_loss: 4805.3423 - val_mse: 4784.2666\n",
            "Epoch 77/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11254.4209 - mse: 11233.2881 - val_loss: 5009.5039 - val_mse: 4988.2720\n",
            "Epoch 78/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9998.4258 - mse: 9977.1729 - val_loss: 5387.8882 - val_mse: 5366.8560\n",
            "Epoch 79/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13302.1211 - mse: 13281.1523 - val_loss: 4831.0327 - val_mse: 4809.9360\n",
            "Epoch 80/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9043.3955 - mse: 9022.2383 - val_loss: 4905.0547 - val_mse: 4884.1299\n",
            "Epoch 81/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9268.3887 - mse: 9247.4941 - val_loss: 4592.3438 - val_mse: 4571.0659\n",
            "Epoch 82/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10852.9121 - mse: 10831.4814 - val_loss: 3916.9934 - val_mse: 3895.5176\n",
            "Epoch 83/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10838.5801 - mse: 10817.0391 - val_loss: 3796.7073 - val_mse: 3775.4622\n",
            "Epoch 84/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10757.7510 - mse: 10736.5752 - val_loss: 3809.1594 - val_mse: 3787.8552\n",
            "Epoch 85/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11126.1182 - mse: 11104.6895 - val_loss: 3643.9224 - val_mse: 3622.1204\n",
            "Epoch 86/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11654.4521 - mse: 11632.5430 - val_loss: 3806.4104 - val_mse: 3784.4622\n",
            "Epoch 87/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12229.7656 - mse: 12207.9697 - val_loss: 4749.7798 - val_mse: 4728.6587\n",
            "Epoch 88/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14573.9072 - mse: 14552.7549 - val_loss: 3644.7837 - val_mse: 3623.0823\n",
            "Epoch 89/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10733.2412 - mse: 10711.6377 - val_loss: 3875.1016 - val_mse: 3853.8257\n",
            "Epoch 90/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13619.9590 - mse: 13598.5693 - val_loss: 3759.1165 - val_mse: 3737.7090\n",
            "Epoch 91/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11156.3633 - mse: 11135.1934 - val_loss: 4568.8354 - val_mse: 4547.6240\n",
            "Epoch 92/3000\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9429.1406 - mse: 9407.7852 - val_loss: 4250.6792 - val_mse: 4229.2246\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Af temperature\n",
            "Training RMSE: 73.37646248988823\n",
            "Training R2: 0.9327077620857182\n",
            "Training MAE: 50.9326328252325\n",
            "Testing RMSE: 59.30546604781719\n",
            "Testing R2: 0.9525506414029362\n",
            "Testing MAE: 41.60491901196931\n",
            "\n",
            "Ms temperature\n",
            "Training RMSE: 57.5648421264485\n",
            "Training R2: 0.9516521758462057\n",
            "Training MAE: 41.87889838667106\n",
            "Testing RMSE: 49.98325993105897\n",
            "Testing R2: 0.9612588325775333\n",
            "Testing MAE: 34.353591256392626\n"
          ]
        }
      ],
      "source": [
        "# importing the required library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Load the training and testing datasets as a CSV\n",
        "x_train = pd.read_csv('x_train.csv').values\n",
        "x_test = pd.read_csv('x_test.csv').values\n",
        "y_train = pd.read_csv('y_train.csv').values\n",
        "y_test = pd.read_csv('y_test.csv').values\n",
        "\n",
        "# Scaling the input features\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# The model architecture\n",
        "ANN_Ms_Af = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=11, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=11, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=2, activation='linear')\n",
        "])\n",
        "\n",
        "# Compiling the model with the Adam optimizer and specified learning rate\n",
        "learning_rate = 0.01\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "ANN_Ms_Af.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# The early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=50,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training the model with the callback and validation data\n",
        "history_with_dropout_l2 = ANN_Ms_Af.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    batch_size=10,\n",
        "    epochs=3000,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Predict using the trained model\n",
        "y_pred_train = ANN_Ms_Af.predict(x_train)\n",
        "y_pred_test = ANN_Ms_Af.predict(x_test)\n",
        "\n",
        "\n",
        "y_train1 = y_train[:, 0]\n",
        "y_train2 = y_train[:, 1]\n",
        "y_test1 = y_test[:, 0]\n",
        "y_test2 = y_test[:, 1]\n",
        "y_pred_train1 = y_pred_train[:, 0]\n",
        "y_pred_train2 = y_pred_train[:, 1]\n",
        "y_pred_test1 = y_pred_test[:, 0]\n",
        "y_pred_test2 = y_pred_test[:, 1]\n",
        "\n",
        "# Calculate RMSE and R-squared values for training set - Af temperature\n",
        "rmse_train1 = np.sqrt(mean_squared_error(y_train1, y_pred_train1))\n",
        "r2_train1 = r2_score(y_train1, y_pred_train1)\n",
        "mae_train1 = mean_absolute_error(y_train1, y_pred_train1)\n",
        "\n",
        "# Calculate RMSE and R-squared values for testing set - Af temperature\n",
        "rmse_test1 = np.sqrt(mean_squared_error(y_test1, y_pred_test1))\n",
        "r2_test1 = r2_score(y_test1, y_pred_test1)\n",
        "mae_test1 = mean_absolute_error(y_test1, y_pred_test1)\n",
        "\n",
        "# Calculate RMSE and R-squared values for training set - Ms temperature\n",
        "rmse_train2 = np.sqrt(mean_squared_error(y_train2, y_pred_train2))\n",
        "r2_train2 = r2_score(y_train2, y_pred_train2)\n",
        "mae_train2 = mean_absolute_error(y_train2, y_pred_train2)\n",
        "\n",
        "# Calculate RMSE and R-squared values for testing set - Ms temperature\n",
        "rmse_test2 = np.sqrt(mean_squared_error(y_test2, y_pred_test2))\n",
        "r2_test2 = r2_score(y_test2, y_pred_test2)\n",
        "mae_test2 = mean_absolute_error(y_test2, y_pred_test2)\n",
        "\n",
        "print(\"Af temperature\")\n",
        "print(f\"Training RMSE: {rmse_train1}\")\n",
        "print(f\"Training R2: {r2_train1}\")\n",
        "print(f\"Training MAE: {mae_train1}\")\n",
        "print(f\"Testing RMSE: {rmse_test1}\")\n",
        "print(f\"Testing R2: {r2_test1}\")\n",
        "print(f\"Testing MAE: {mae_test1}\")\n",
        "\n",
        "print(\"\\nMs temperature\")\n",
        "print(f\"Training RMSE: {rmse_train2}\")\n",
        "print(f\"Training R2: {r2_train2}\")\n",
        "print(f\"Training MAE: {mae_train2}\")\n",
        "print(f\"Testing RMSE: {rmse_test2}\")\n",
        "print(f\"Testing R2: {r2_test2}\")\n",
        "print(f\"Testing MAE: {mae_test2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Regression (SVR)**"
      ],
      "metadata": {
        "id": "poTQPHW6Nj9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A_f Temperature Prediction**"
      ],
      "metadata": {
        "id": "tx0al878Ns7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Loading the training and testing datasets as a CSV\n",
        "x_train = pd.read_csv('x_train.csv').values\n",
        "x_test = pd.read_csv('x_test.csv').values\n",
        "y_train = pd.read_csv('y_train.csv').iloc[:, 0].values\n",
        "y_test = pd.read_csv('y_test.csv').iloc[:, 0].values\n",
        "\n",
        "# Creating the SVR model\n",
        "svr = SVR(C=0.1, epsilon=0.1, gamma=0.01, kernel='poly')\n",
        "\n",
        "# Training the model\n",
        "svr.fit(x_train, y_train)\n",
        "\n",
        "y_train_pred = svr.predict(x_train)\n",
        "y_test_pred = svr.predict(x_test)\n",
        "\n",
        "# Calculate RMSE, R2 and MAE for training and testing sets\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "mae_test = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training R2: {r2_train}\")\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "\n",
        "print(f\"Testing RMSE: {rmse_test}\")\n",
        "print(f\"Testing R2: {r2_test}\")\n",
        "print(f\"Testing MAE: {mae_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqQ2ZAgEHQOR",
        "outputId": "6dd787a0-6d89-4cfe-ba27-856c9c6096ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 45.40077061923865\n",
            "Training R2: 0.9742381006034877\n",
            "Training MAE: 22.22994203597298\n",
            "Testing RMSE: 58.41371513568413\n",
            "Testing R2: 0.9539668645710594\n",
            "Testing MAE: 33.81943580300311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M_s Temperature Prediction**"
      ],
      "metadata": {
        "id": "lLVvQMn-N2D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Loading the training and testing datasets as a CSV\n",
        "x_train = pd.read_csv('x_train.csv').values\n",
        "x_test = pd.read_csv('x_test.csv').values\n",
        "y_train = pd.read_csv('y_train.csv').iloc[:, 1].values\n",
        "y_test = pd.read_csv('y_test.csv').iloc[:, 1].values\n",
        "\n",
        "# Creating the SVR model\n",
        "svr = SVR(C=0.1, epsilon=0.1, gamma=0.01, kernel='poly')\n",
        "\n",
        "# Training the model\n",
        "svr.fit(x_train, y_train)\n",
        "\n",
        "y_train_pred = svr.predict(x_train)\n",
        "y_test_pred = svr.predict(x_test)\n",
        "\n",
        "# Calculate RMSE, R2 and MAE for training and testing sets\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "mae_test = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training R2: {r2_train}\")\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "\n",
        "print(f\"Testing RMSE: {rmse_test}\")\n",
        "print(f\"Testing R2: {r2_test}\")\n",
        "print(f\"Testing MAE: {mae_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1s0Go2YJQmC",
        "outputId": "05b9cf5d-87c3-4662-c80a-c5b93a29c285"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 38.20814301584975\n",
            "Training R2: 0.9787002670004175\n",
            "Training MAE: 19.12652526421451\n",
            "Testing RMSE: 44.331110186470546\n",
            "Testing R2: 0.9695252070697044\n",
            "Testing MAE: 28.405548413034108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest Regression (RFR)**"
      ],
      "metadata": {
        "id": "BE-Gss4wN7Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A_f Temperature Prediction**"
      ],
      "metadata": {
        "id": "YtCaaxRtOBVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Loading the training and testing datasets as a CSV\n",
        "x_train = pd.read_csv('x_train.csv').values\n",
        "x_test = pd.read_csv('x_test.csv').values\n",
        "y_train = pd.read_csv('y_train.csv').iloc[:, 0].values\n",
        "y_test = pd.read_csv('y_test.csv').iloc[:, 0].values\n",
        "\n",
        "# Scaling the input features\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# Define the RandomForestRegressor model\n",
        "regressor = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=0)\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "y_train_pred = regressor.predict(x_train)\n",
        "y_test_pred = regressor.predict(x_test)\n",
        "\n",
        "# Calculate RMSE and R2 for training and testing sets\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "mae_test = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training R2: {r2_train}\")\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "\n",
        "print(f\"Testing RMSE: {rmse_test}\")\n",
        "print(f\"Testing R2: {r2_test}\")\n",
        "print(f\"Testing MAE: {mae_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBmcjd7kK456",
        "outputId": "4e07c330-4064-472a-d4e2-3264c20ca56e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 33.57848214580051\n",
            "Training R2: 0.9859079702156042\n",
            "Training MAE: 18.814400759088848\n",
            "Testing RMSE: 94.91375137377881\n",
            "Testing R2: 0.8784656122437836\n",
            "Testing MAE: 49.001858701045556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M_s Temperature Prediction**"
      ],
      "metadata": {
        "id": "Y0c__23wON55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Loading the training and testing datasets as a CSV\n",
        "x_train = pd.read_csv('x_train.csv').values\n",
        "x_test = pd.read_csv('x_test.csv').values\n",
        "y_train = pd.read_csv('y_train.csv').iloc[:, 1].values\n",
        "y_test = pd.read_csv('y_test.csv').iloc[:, 1].values\n",
        "\n",
        "# Scaling the input features\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# Define the RandomForestRegressor model\n",
        "regressor = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=0)\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "y_train_pred = regressor.predict(x_train)\n",
        "y_test_pred = regressor.predict(x_test)\n",
        "\n",
        "# Calculate RMSE and R2 for training and testing sets\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "mae_test = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training R2: {r2_train}\")\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "\n",
        "print(f\"Testing RMSE: {rmse_test}\")\n",
        "print(f\"Testing R2: {r2_test}\")\n",
        "print(f\"Testing MAE: {mae_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nemnaLRtNF-W",
        "outputId": "93aa9cba-6c5d-4276-d7ff-d8675a32d4fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 28.920837137587334\n",
            "Training R2: 0.9877965108984416\n",
            "Training MAE: 15.623220523675464\n",
            "Testing RMSE: 69.07870780981239\n",
            "Testing R2: 0.926003367075713\n",
            "Testing MAE: 38.76906851491589\n"
          ]
        }
      ]
    }
  ]
}